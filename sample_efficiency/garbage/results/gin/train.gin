# Parameters for AdamOptimizer:
# ==============================================================================
AdamOptimizer.beta1 = 0.9
AdamOptimizer.beta2 = 0.999
AdamOptimizer.epsilon = 1e-08
AdamOptimizer.learning_rate = 0.0001
AdamOptimizer.name = 'Adam'
AdamOptimizer.use_locking = False

# Parameters for conv_encoder:
# ==============================================================================
# None.

# Parameters for dataset:
# ==============================================================================
dataset.name = '3dshapes_model_s1000_vae_1602'

# Parameters for decoder:
# ==============================================================================
decoder.decoder_fn = @deconv_decoder

# Parameters for deconv_decoder:
# ==============================================================================
# None.

# Parameters for encoder:
# ==============================================================================
encoder.encoder_fn = @conv_encoder
encoder.num_latent = 10

# Parameters for gaussian_encoder_model.export_as_tf_hub:
# ==============================================================================
gaussian_encoder_model.export_as_tf_hub.drop_collections = None

# Parameters for l2_loss:
# ==============================================================================
# None.

# Parameters for model:
# ==============================================================================
model.batch_size = 64
model.eval_steps = 1000
model.model = @vae()
model.model_num = None
model.name = ''
model.random_seed = 1602
model.training_steps = 100000

# Parameters for reconstruction_loss:
# ==============================================================================
reconstruction_loss.activation = 'logits'
reconstruction_loss.loss_fn = @l2_loss

# Parameters for vae:
# ==============================================================================
vae.beta = 1

# Parameters for vae_optimizer:
# ==============================================================================
vae_optimizer.learning_rate = None
vae_optimizer.optimizer_fn = @AdamOptimizer
